import copy
from typing import Dict, List, Tuple, Union, Literal
from dataclasses import dataclass 

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import mean_squared_error
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from tensorflow.keras import Model, Sequential, activations, regularizers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dense
from tensorflow.keras.initializers import HeNormal
import tensorflow as tf
import matplotlib.pyplot as plt

"""
================
CODE FOR SPLIT 1
================
"""



def generate_dataset(
    n_samples: int = 100,
    means: np.ndarray = np.array([[4.0, -2.0], [-2.0, 3.0]]),
    standard_deviations: List[float] = [3.5, 5.0],
    seed: int = 20250122,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Generates a synthetic dataset with two classes,
    where each class follows a Gaussian-like distribution.

    Parameters:
    ----------
    n_samples : int, optional
        The number of samples to generate per class. Default is 100.
    means : np.ndarray, optional
        A 2D array where each row represents the mean (center) of a class
        in 2D space. Default is np.array([[4.0, -2.0], [-2.0, 3.0]]).
    standard_deviations : List[float], optional
        A list of standard deviations for each class. Each standard deviation
        determines the spread of the points around the corresponding mean.
        Default is [3.5, 5.0].
    seed : int, optional
        Random seed for reproducibility. Default is 20250122.

    Returns:
    -------
    Tuple[np.ndarray, np.ndarray]
        A tuple containing:
        - data: A 2D NumPy array of shape (n_samples * num_classes, 2), where
          each row is a point in 2D space.
        - labels: A 1D NumPy array of shape (n_samples * num_classes,),
          where each element is the class label (0, 1, ...).

    Raises:
    ------
    AssertionError
        If the number of class means does not match the number of standard
        deviations or if `means` is not a 2D array.

    Notes:
    -----
    - The data points for each class are generated by adding Gaussian noise
      to the specified class mean, scaled by the corresponding standard
      deviation.
    - The generated data and labels are shuffled to ensure randomness.
    """

    # Ensure two-dimensionality
    assert means.ndim == 2
    # Ensure same amount of classes
    assert len(means) == len(standard_deviations)

    # Initialise data points and labels
    data = []
    labels = []
    generator = np.random.default_rng(seed=seed)

    for i in range(len(means)):
        # Generate random points with specific mean and standard deviation
        points = generator.random((2, n_samples)) * standard_deviations[i] + means[
            i
        ].reshape(2, 1)
        # Add points to data
        data.append(points)
        # Add same label for the generated points
        labels.append(np.full((n_samples), i, dtype=int))

    # Concatenate all data points and labels
    data = np.hstack(data)
    labels = np.hstack(labels)

    # Shuffle data points and labels with same permutation
    indexes = generator.permutation(n_samples * len(means))
    data = data[:, indexes]
    labels = labels[indexes]

    return data, labels


"""
================
CODE FOR SPLIT 2
================
"""

def generate_gaussian_data(
    n: int
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Generate n data samples following the given Gaussian function.
    Returns:
        patterns: (2, n) array of input data
        targets: (1, n) array of output data
    """

    x = np.linspace(-5, 5, int(np.sqrt(n)))
    y = np.linspace(-5, 5, int(np.sqrt(n)))
    xx, yy = np.meshgrid(x, y)

    # Compute the Gaussian function
    z = np.exp(-(xx**2 + yy**2) / 10) - 0.5
    patterns = np.vstack((xx.ravel(), yy.ravel()))
    targets = z.ravel().reshape(1, -1)
    
    return patterns.T, targets.T


@dataclass 
class ModelResult: 
    "Dataclass to store model and its performance metrics."
    model: Sequential
    mse_train: float 
    mse_val: float
    mse_overall: float


def load_MLP_regressor(
    n_nodes: int=10,
    optimizer='adam', 
    loss='mse',
    use_kernel_initializer: bool=False
) -> Sequential:
    "Loads a 2-layer Keras MLP with relu activation function."
    tf.random.set_seed(42)
    if use_kernel_initializer: 
        model = Sequential([
            Dense(n_nodes, activation='relu', kernel_initializer=HeNormal(), input_shape=(2,)),
            Dense(n_nodes, activation='relu', kernel_initializer=HeNormal()),
            Dense(1, activation='linear')
        ])
    else: 
        model = Sequential([
            Dense(n_nodes, activation='relu', input_shape=(2,)),
            Dense(n_nodes, activation='relu'),
            Dense(1, activation='linear')
        ])
    model.compile(optimizer=optimizer, loss=loss)
    return model


def split_dataset(
    X: np.ndarray, 
    y: np.ndarray,
    training_fraction: float = 0.6,
    use_seed: bool = True
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: 
    if use_seed:
        np.random.seed(42)

    num_samples = X.shape[0]
    indices = np.arange(num_samples)

    val_size = int(num_samples * training_fraction)
    val_indices = indices[:val_size]
    train_indices = indices[val_size:]

    X_train, X_val = X[train_indices], X[val_indices]
    y_train, y_val = y[train_indices], y[val_indices]

    return X_train, X_val, y_train, y_val


def evaluate(
    model: Sequential, 
    X: np.ndarray, 
    y: np.ndarray,
    X_train: np.ndarray = None, 
    X_val: np.ndarray = None, 
    y_train: np.ndarray = None,
    y_val: np.ndarray = None,
    training_fraction: float = 0.6
) -> Tuple[float, float, float]:
    """
    Calculates the MSE of the training, validation, and overall dataset.
    Training and validation splitting can be provided manually.
    Otherwise, the function automatically splits the dataset (same seed). 
    """
    tf.random.set_seed(42)
    if (X_train is None) or (X_val is None) or (y_train is None) or (y_val is None):
        print("splitting ownself")
        X_train, X_val, y_train, y_val = split_dataset(X, y, training_fraction)
    mse_train = model.evaluate(X_train, y_train)
    mse_val = model.evaluate(X_val, y_val)
    mse_overall = model.evaluate(X, y)

    return mse_train, mse_val, mse_overall


def plot_mse_results(
    x_axis: Literal["nodes", "training_fraction"],
    list_avg_mse_train: List[float],
    list_avg_mse_val: List[float],
    list_avg_mse_overall: List[float],
    list_stdev_mse_train: List[float],
    list_stdev_mse_val: List[float],
    list_stdev_mse_overall: List[float],
    fig_size = (10,6)
) -> None: 
    assert x_axis in ["nodes", "training_fraction"]

    plt.figure(figsize=fig_size)
    if x_axis == "nodes":
        x_axis_range = list(range(1, 26, 1))
        plt.xlabel('Number of Nodes in MLP')
        plt.ylabel('Mean Squared Error (MSE)')
        plt.title('MSE vs Number of Nodes in MLP (Mean and SD)')
    elif x_axis == "training_fraction":
        x_axis_range = list(range(20, 81, 10))
        plt.xlabel('Training Fraction (%)')
        plt.ylabel('Mean Squared Error (MSE)')
        plt.title('MSE vs Training Fraction (Mean and SD)')

    plt.errorbar(x_axis_range, list_avg_mse_train, yerr=list_stdev_mse_train, fmt='o-', capsize=5, label="MSE Train", color='b')
    plt.errorbar(x_axis_range, list_avg_mse_val, yerr=list_stdev_mse_val, fmt='o-', capsize=5, label="MSE Val", color='r')
    plt.errorbar(x_axis_range, list_avg_mse_overall, yerr=list_stdev_mse_overall, fmt='o-', capsize=5, label="MSE Overall", color='g')

    plt.legend()
    plt.grid(True)
    plt.show()



"""
================
CODE FOR SPLIT 3
================
"""

def mackey_glass_time_series(t: float) -> np.ndarray:
    y = np.empty(t, dtype=np.float64)
    y[0] = 1.5

    for i in range(1, len(y)):
        delay = 0 if i - 25 < 0 else y[i - 25]
        previous = y[i - 1]

        y[i] = previous + (0.2 * delay) / (1 + delay**10) - 0.1 * previous

    return y


class SKTimeSeriesMLP:
    def __init__(
        self,
        data_dim=5,
        hidden_nodes=(8, 6),  # matching the working example structure
        n_epochs=1000,
        patience=10,
        early_stopping=True,
        lmbda=0.01,
    ):
        self.data_dim = data_dim
        self.hidden_nodes = hidden_nodes
        self.n_epochs = n_epochs
        self.patience = patience
        self.early_stopping = early_stopping
        self.lmbda = lmbda

        # Initialize model based on early_stopping flag
        if early_stopping:
            alpha = 0  # No regularization when using early stopping
        else:
            alpha = lmbda  # Use L2 regularization when no early stopping

        self.model = MLPRegressor(
            hidden_layer_sizes=hidden_nodes,
            activation="logistic",
            solver="adam",
            alpha=alpha,
            batch_size=32,  # Match the working code's batch size
            learning_rate="constant",  # More similar to Keras implementation
            learning_rate_init=0.001,
            max_iter=1,
            warm_start=True,
            random_state=42,
        )

        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.best_model = None
        self.best_loss = np.inf

    def fit(self, X_train, y_train, X_val, y_val):
        # Scale data
        X_train_scaled = self.scaler_X.fit_transform(X_train)
        X_val_scaled = self.scaler_X.transform(X_val)

        y_train_scaled = self.scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()
        y_val_scaled = self.scaler_y.transform(y_val.reshape(-1, 1)).ravel()

        history = {"loss": [], "val_loss": [], "epoch": []}
        no_improvement_count = 0

        for epoch in range(self.n_epochs):
            self.model.partial_fit(X_train_scaled, y_train_scaled)

            # Calculate losses
            train_loss = mean_squared_error(
                y_train_scaled, self.model.predict(X_train_scaled)
            )
            val_loss = mean_squared_error(
                y_val_scaled, self.model.predict(X_val_scaled)
            )

            history["loss"].append(train_loss)
            history["val_loss"].append(val_loss)
            history["epoch"].append(epoch)

            # Early stopping logic
            if self.early_stopping:
                if val_loss < (self.best_loss - 1e-4):
                    self.best_loss = val_loss
                    self.best_model = copy.deepcopy(self.model)
                    no_improvement_count = 0
                else:
                    no_improvement_count += 1

                if no_improvement_count >= self.patience:
                    print(f"Early stopping at epoch {epoch}")
                    break

        # Restore best model if using early stopping
        if self.early_stopping and self.best_model is not None:
            self.model = self.best_model

        return history

    def predict(self, X):
        X_scaled = self.scaler_X.transform(X)
        predictions_scaled = self.model.predict(X_scaled)
        return self.scaler_y.inverse_transform(
            predictions_scaled.reshape(-1, 1)
        ).ravel()


class TimeSeriesMLP(Model):
    def __init__(
        self,
        data_dim=5,
        output_dim=1,
        hidden_nodes=(8, 6),
        n_epochs=500,
        patience=10,
        early_stopping=True,
        lmbda=0.0001,
    ):
        super(TimeSeriesMLP, self).__init__()

        self.hidden_nodes = hidden_nodes
        self.hidden_n = len(hidden_nodes)
        self.data_dim = data_dim
        self.output_dim = output_dim
        self.patience = patience
        self.n_epochs = n_epochs
        self.early_stopping = early_stopping
        self.lmbda = lmbda

        self.build_model()

    def build_model(self):
        self.model = Sequential()

        # Input layer
        self.model.add(
            Dense(units=self.data_dim, activation=activations.linear, use_bias=True)
        )

        if self.early_stopping:
            # Hidden layers without regularization when using early stopping
            for i in range(self.hidden_n):
                self.model.add(
                    Dense(
                        units=self.hidden_nodes[i],
                        activation=activations.sigmoid,
                        use_bias=True,
                    )
                )

            # Output layer without regularization
            self.model.add(
                Dense(
                    units=self.output_dim, activation=activations.linear, use_bias=True
                )
            )
        else:
            # Hidden layers with L2 regularization when not using early stopping
            for i in range(self.hidden_n):
                self.model.add(
                    Dense(
                        units=self.hidden_nodes[i],
                        activation=activations.sigmoid,
                        use_bias=True,
                        kernel_regularizer=regularizers.l2(self.lmbda),
                    )
                )

            # Output layer with L2 regularization
            self.model.add(
                Dense(
                    units=self.output_dim,
                    activation=activations.linear,
                    use_bias=True,
                    kernel_regularizer=regularizers.l2(self.lmbda),
                )
            )

        # Compile model
        self.model.compile(optimizer="adam", loss="mse")

    def fit(self, X_train, y_train, X_val, y_val):
        if self.early_stopping:
            # Define early stopping callback
            early_stopping = EarlyStopping(
                monitor="val_loss",
                patience=self.patience,
                restore_best_weights=True,
                verbose=0,
            )

            history = self.model.fit(
                X_train,
                y_train,
                validation_data=(X_val, y_val),
                epochs=self.n_epochs,
                batch_size=32,
                verbose=1,
                callbacks=[early_stopping],
            )
        else:
            history = self.model.fit(
                X_train,
                y_train,
                validation_data=(X_val, y_val),
                epochs=self.n_epochs,
                batch_size=32,
                verbose=1,
            )

        return history.history

    def predict(self, X):
        return self.model.predict(X, verbose=0)

    def evaluate(self, X_eval, y_eval):
        return self.model.evaluate(X_eval, y_eval, verbose=0)


def plot_training_histories(histories):
    """
    Plot training histories for different MLP configurations

    Parameters:
    histories: dict
        Dictionary where keys are tuples of (n_nodes_layer1, n_nodes_layer2)
        and values are dictionaries containing 'train_loss', 'val_loss', and 'epoch'
    """
    # Set up the plotting style
    plt.style.use("seaborn-v0_8-whitegrid")
    plt.rcParams.update({"font.size": 20})
    fig, ax = plt.subplots(figsize=(12, 8))

    # Color palette for different configurations
    n_configs = len(histories)
    colors = sns.color_palette("husl", n_configs)

    # Find the best configuration based on minimum validation loss
    best_config = min(histories.items(), key=lambda x: x[1][("best_loss", "mean")][-1])

    # Plot each configuration
    for (config, history), color in zip(histories.items(), colors):
        epochs = history[("epoch", "")]

        # Plot with higher alpha if it's the best configuration
        alpha = 1.0 if config == best_config[0] else 0.25

        # Plot training loss
        ax.plot(
            epochs,
            history[("train_loss", "mean")],
            color=color,
            linestyle="-",
            alpha=alpha,
            label=f"Train {config}",
        )

        # Plot validation loss
        ax.plot(
            epochs,
            history[("val_loss", "mean")],
            color=color,
            linestyle="--",
            marker="o",
            markersize=4,
            markevery=5,
            alpha=alpha,
            label=f"Val {config}",
        )

    # Highlight the best configuration in the legend
    legend = ax.legend(
        title="Configuration (value of $\\lambda$)",
        bbox_to_anchor=(1.05, 1),
        loc="upper left",
    )
    for text in legend.get_texts():
        if str(best_config[0]) in text.get_text():
            text.set_weight("bold")

    # Customize the plot
    ax.set_xlabel("Epoch")
    ax.set_ylabel("Average MSE")
    ax.set_title("MLP Training History for Different Layer Configurations")
    ax.grid(True, alpha=0.3)

    # Add text box with best configuration details
    best_final_val = best_config[1][("val_loss", "mean")][-1]
    textstr = f"Best Config: {best_config[0]}\nFinal Val Loss: {best_final_val:.4f}"
    props = dict(boxstyle="round", facecolor="wheat", alpha=0.5)
    ax.text(
        0.02,
        0.98,
        textstr,
        transform=ax.transAxes,
        fontsize=10,
        verticalalignment="top",
        bbox=props,
    )

    # Adjust layout to prevent label cutoff
    plt.tight_layout()
    return fig, ax
