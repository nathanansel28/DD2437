from typing import List, Tuple, Union

import matplotlib.pyplot as plt
import numpy as np


def generate_dataset(
    n_samples: int = 100,
    means: np.ndarray = np.array([[4.0, -2.0], [-2.0, 3.0]]),
    standard_deviations: List[float] = [3.5, 5.0],
    seed: int = 20250122,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Generates a synthetic dataset with two classes,
    where each class follows a Gaussian-like distribution.

    Parameters:
    ----------
    n_samples : int, optional
        The number of samples to generate per class. Default is 100.
    means : np.ndarray, optional
        A 2D array where each row represents the mean (center) of a class
        in 2D space. Default is np.array([[4.0, -2.0], [-2.0, 3.0]]).
    standard_deviations : List[float], optional
        A list of standard deviations for each class. Each standard deviation
        determines the spread of the points around the corresponding mean.
        Default is [3.5, 5.0].
    seed : int, optional
        Random seed for reproducibility. Default is 20250122.

    Returns:
    -------
    Tuple[np.ndarray, np.ndarray]
        A tuple containing:
        - data: A 2D NumPy array of shape (n_samples * num_classes, 2), where
          each row is a point in 2D space.
        - labels: A 1D NumPy array of shape (n_samples * num_classes,),
          where each element is the class label (0, 1, ...).

    Raises:
    ------
    AssertionError
        If the number of class means does not match the number of standard
        deviations or if `means` is not a 2D array.

    Notes:
    -----
    - The data points for each class are generated by adding Gaussian noise
      to the specified class mean, scaled by the corresponding standard
      deviation.
    - The generated data and labels are shuffled to ensure randomness.
    """

    # Ensure two-dimensionality
    assert means.ndim == 2
    # Ensure same amount of classes
    assert len(means) == len(standard_deviations)

    # Initialise data points and labels
    data = []
    labels = []
    generator = np.random.default_rng(seed=seed)

    for i in range(len(means)):
        # Generate random points with specific mean and standard deviation
        points = generator.random((2, n_samples)) * standard_deviations[i] + means[
            i
        ].reshape(2, 1)
        # Add points to data
        data.append(points)
        # Add same label for the generated points
        labels.append(np.full((n_samples), i, dtype=int))

    # Concatenate all data points and labels
    data = np.hstack(data)
    labels = np.hstack(labels)

    # Shuffle data points and labels with same permutation
    indexes = generator.permutation(n_samples * len(means))
    data = data[:, indexes]
    labels = labels[indexes]

    return data, labels


def plot_decision_boundary(
    weights: np.ndarray, data: np.ndarray, labels: np.ndarray, title: str
):
    """
    Plots a 2D dataset with its corresponding decision boundary.

    Parameters:
    ----------
    weights : np.ndarray
        A 1D NumPy array of shape (3,) or (,3) containing the weights of the decision
        boundary equation: [w1, w2, b], where:
        - w1 and w2 are the coefficients for the features X1 and X2.
        - b is the bias term.
    data : np.ndarray
        A 2D NumPy array of shape (2, n_samples) containing the dataset. Each
        column represents a data point, where the first row contains the X1
        coordinates, and the second row contains the X2 coordinates.
    labels : np.ndarray
        A 1D NumPy array of shape (n_samples,) containing the class labels
        (0 or 1) for each data point.
    title : str
        The title of the plot.

    Returns:
    -------
    None
        This function does not return anything. It displays the plot.

    Notes:
    -----
    - The decision boundary is defined by the equation: `w1 * X1 + w2 * X2 + b = 0`.
      If w2 is 0, a vertical line is plotted at `X1 = -b / w1`.
    - The dataset points are colored based on their labels:
        - "Class A" (label 0) is displayed in blue.
        - "Class B" (label 1) is displayed in orange.
    - The plot includes axes with gridlines for better visualization.
    """
    # Unpack weights (w1, w2, b)
    w1, w2, b = weights

    # Generate X1 values (spanning the range of the dataset)
    x1_range = np.linspace(data[0, :].min() - 1, data[0, :].max() + 1, 100)

    # Calculate x2 values using the decision boundary equation
    if w2 != 0:  # Avoid division by zero
        x2_range = -(w1 * x1_range + b) / w2
    else:
        x2_range = np.full_like(x1_range, -b / w1)  # Vertical line if w2 == 0

    # Plot the data points
    plt.scatter(
        data[0, labels != 1],
        data[1, labels != 1],
        label="Class A",
        alpha=0.7,
        color="blue",
    )
    plt.scatter(
        data[0, labels == 1],
        data[
            1,
            labels == 1,
        ],
        label="Class B",
        alpha=0.7,
        color="orange",
    )

    # Plot the decision boundary
    plt.plot(x1_range, x2_range, "k--", label="Decision boundary")

    # Customize the plot
    plt.grid(True, linestyle="--")
    plt.xlabel("X1")
    plt.ylabel("X2")
    plt.legend()
    plt.title(title)
    plt.tight_layout()
    plt.show()


def add_bias(X: np.ndarray) -> np.ndarray:
    """
    Adds a bias term to the input data by appending a row of ones.

    Parameters:
    ----------
    X : np.ndarray
        A 2D NumPy array of shape (n_features, n_samples), where each column
        represents a data point.

    Returns:
    -------
    np.ndarray
        A 2D NumPy array of shape (n_features + 1, n_samples) with an
        additional bias row of ones.
    """
    return np.vstack((X, np.ones((1, X.shape[1]))))


class PerceptronClassifier:
    def __init__(self) -> None:
        """
        Initializes the PerceptronClassifier with no weights. The weights
        are initialized during training using the `fit` method.
        """
        self.weights: np.ndarray = None

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        learn_rate: float = 0.25,
        epochs: int = 10,
        batch: bool = False,
    ) -> None:
        """
        Trains the perceptron model using the provided training data and labels.

        Parameters:
        ----------
        X : np.ndarray
            A 2D NumPy array of shape (n_features, n_samples) containing the
            training data. Each column represents a data point.
        y : np.ndarray
            A 1D or 2D NumPy array of shape (n_samples,) or (n_samples, 1)
            containing the class labels for each data point. For binary classification,
            labels should be 0 or 1.
        learn_rate : float, optional
            The learning rate for weight updates. Default is 0.25.
        epochs : int, optional
            The number of passes over the training data. Default is 10.
        batch : bool, optional
            If True, updates are accumulated over all samples in an epoch before being applied (batch learning).
            If False, weights are updated for each sample individually (online learning). Default is False.

        Returns:
        -------
        None
            Trains the model and updates the `weights` attribute in place.

        Notes:
        -----
        - Weights are initialized randomly in the range [-0.05, 0.05].
        - For binary classification, `weights` will have shape (n_features + 1, 1).
        - For multi-class classification, `weights` will have shape (n_features + 1, n_classes).
        """
        inputs = add_bias(X)
        labels = y = (
            y.reshape(-1, 1) if y.ndim == 1 else y
        )  # Ensure y is (1, n_samples)

        rndg = np.random.default_rng(seed=20250122)
        # Initialise weight randomly with mean 0.05 and standard deviation 0.05
        num_classes = len(np.unique(y))
        self.weights = (
            rndg.random((X.shape[0] + 1, 1 if num_classes == 2 else num_classes)) * 0.1
            - 0.05
        )

        for _ in range(epochs):
            if batch:
                # Get predictions
                preds = self.predict(X)
                # Compute delta weights
                delta = learn_rate * np.dot(inputs, labels - preds)

                self.weights += delta
            else:
                for datapoint, label in zip(X.T, labels):
                    error = label.item() - self.predict(datapoint.reshape(-1, 1)).item()
                    delta = learn_rate * error * add_bias(datapoint.reshape(-1, 1))

                    self.weights += delta

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Predicts class labels for the given input data.

        Parameters:
        ----------
        X : np.ndarray
            A 2D NumPy array of shape (n_features, n_samples) containing the
            input data. Each column represents a data point.

        Returns:
        -------
        np.ndarray
            A 1D NumPy array of shape (n_samples,) for multi-class classification,
            or a 2D array of shape (n_samples, 1) for binary classification.
            Binary predictions are 0 or 1.

        Notes:
        -----
        - For binary classification, activations are thresholded at 0 to predict labels.
        - For multi-class classification, the predicted label corresponds to the
          index of the maximum activation.
        """
        inputs = add_bias(X)

        # Compute activations
        activations = np.dot(np.transpose(self.weights), inputs)
        # Threshold the activations
        if self.weights.shape[1] == 1:  # Binary classification
            return np.where(activations > 0, 1, 0).reshape((-1, 1))
        return np.argmax(activations, axis=0)  # Multi-class classification


class DeltaRuleClassifier:
    def __init__(self) -> None:
        """
        Initializes the DeltaRuleClassifier with no weights. Weights are initialized
        during training using the `fit` method.
        """
        self.weights = None

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        learn_rate: float = 0.001,
        epochs: int = 20,
        batch: bool = True,
    ) -> None:
        """
        Trains the classifier on the input data and labels using gradient descent.

        Parameters:
        ----------
        X : np.ndarray
            A 2D NumPy array of shape (n_features, n_samples) containing the training data.
            Each column is a data point.
        y : np.ndarray
            A 1D or 2D NumPy array of shape (n_samples,) or (n_samples, n_classes)
            containing the true labels for each data point. Binary labels should be
            represented as 1 and -1 for binary classification.
        learn_rate : float, optional
            The learning rate for weight updates. Default is 0.001.
        epochs : int, optional
            The number of iterations over the training data. Default is 20.
        batch : bool, optional
            If True, updates are accumulated over all samples before being applied to
            the weights (batch learning). If False, weights are updated for each sample
            individually (online learning). Default is True.

        Returns:
        -------
        None
            The trained weights are stored in the `weights` attribute.

        Notes:
        -----
        - The weights are initialized randomly in the range [-0.05, 0.05].
        - For binary classification, `weights` will have shape (n_features + 1, 1).
        - For multi-class classification, `weights` will have shape
          (n_features + 1, n_classes).
        """
        inputs = add_bias(X)
        labels = y.reshape(-1, 1) if y.ndim == 1 else y  # Ensure y is (1, n_samples)

        rndg = np.random.default_rng(seed=None)
        # Initialise weight randomly with mean 0.05 and standard deviation 0.05
        num_classes = len(np.unique(y))
        self.weights = (
            rndg.random((X.shape[0] + 1, 1 if num_classes == 2 else num_classes)) * 0.1
            - 0.05
        )

        for _ in range(epochs):
            if batch:
                # Compute error
                delta = -learn_rate * ((self.weights.T @ inputs) - labels.T) @ inputs.T

                self.weights += delta.reshape(-1, 1)
            else:
                for datapoint, label in zip(inputs.T, labels):
                    error = label.item() - (self.weights.T @ datapoint).item()
                    delta = learn_rate * (datapoint.reshape(-1, 1) * error)

                    self.weights += delta

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Predicts class labels for the input data.

        Parameters:
        ----------
        X : np.ndarray
            A 2D NumPy array of shape (n_features, n_samples) containing the input data.
            Each column is a data point.

        Returns:
        -------
        np.ndarray
            A 2D NumPy array of shape (n_samples, 1) for binary classification,
            containing the predicted labels (1 or -1).
            For multi-class classification, returns a 1D NumPy array of shape (n_samples,)
            containing the predicted class indices (0, 1, ..., n_classes-1).

        Notes:
        -----
        - For binary classification, activations are thresholded at 0.
        - For multi-class classification, the predicted class corresponds to the index
          of the maximum activation for each sample.
        """
        inputs = add_bias(X)

        # Compute activations
        activations = np.dot(np.transpose(self.weights), inputs)
        # Threshold the activations
        if self.weights.shape[1] == 1:  # Binary classification
            return np.where(activations > 0, 1, -1).reshape((-1, 1))
        return np.argmax(activations, axis=0)  # Multi-class classification


def plot_datasets(
    original_data,
    original_labels,
    subsampled_data=None,
    subsampled_labels=None,
    title: str = "",
) -> None:
    """
    This is for task 3.

    Plot the dataset with optional subsampled data.

    If subsampled data is provided, the original data is shown with lower opacity.
    """

    def scatter_data(data, labels, alpha, label_suffix="", edgecolor=None, s=None):
        plt.scatter(
            data[0, labels != 1],
            data[1, labels != 1],
            color="blue",
            alpha=alpha,
            label=f"Class A {label_suffix}",
            edgecolor=edgecolor,
            s=s,
        )
        plt.scatter(
            data[0, labels == 1],
            data[1, labels == 1],
            color="orange",
            alpha=alpha,
            label=f"Class B {label_suffix}",
            edgecolor=edgecolor,
            s=s,
        )

    if subsampled_data is not None and subsampled_labels is not None:
        scatter_data(
            original_data, original_labels, alpha=0.1, label_suffix="(Original)"
        )
        scatter_data(
            subsampled_data,
            subsampled_labels,
            alpha=0.7,
            label_suffix="(Subsampled)",
            edgecolor="k",
            s=50,
        )
    else:
        scatter_data(original_data, original_labels, alpha=0.7)

    plt.title(title)
    plt.xlabel("X1")
    plt.ylabel("X2")
    plt.legend()
    plt.grid(True, linestyle="--")
    plt.show()


def generate_subsample_dataset(
    n_samples: int = 100,
    mA: np.array = np.array([1.0, 0.3]),
    mB: np.array = np.array([0.0, -0.1]),
    sigmaA: float = 0.2,
    sigmaB: float = 0.3,
    seed: int = 20250122,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Generates the dataset required for part 2 of task 3.1.3
    """
    np.random.seed(seed)

    classA = np.zeros((2, n_samples))
    classA[0, : n_samples // 2] = np.random.randn(n_samples // 2) * sigmaA - mA[0]
    classA[0, n_samples // 2 :] = np.random.randn(n_samples // 2) * sigmaA + mA[0]
    classA[1, :] = np.random.randn(n_samples) * sigmaA + mA[1]

    classB = np.zeros((2, n_samples))
    classB[0, :] = np.random.randn(n_samples) * sigmaB + mB[0]
    classB[1, :] = np.random.randn(n_samples) * sigmaB + mB[1]

    labels_A = np.zeros(n_samples, dtype=int)
    labels_B = np.ones(n_samples, dtype=int)

    data = np.hstack((classA, classB))
    labels = np.hstack((labels_A, labels_B))

    return data, labels, classA, labels_A, classB, labels_B


def subsample_random(
    data: np.ndarray,
    labels: np.ndarray,
    fraction: float,
    class_label: Union[int, None] = None,
    seed: int = 42,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Removes a fraction of the dataset randomly, optionally filtered by class_label.

    Parameters:
        data (np.ndarray): The dataset of shape (features, samples).
        labels (np.ndarray): Labels corresponding to the samples (1D array).
        fraction (float): Fraction of samples to be removed (0 < fraction < 1).
        class_label (Union[int, None]): If specified, only subsamples from this class.
        seed (int): Random seed for reproducibility.

    Returns:
        Tuple[np.ndarray, np.ndarray]: Subsampled data and corresponding labels.
    """
    np.random.seed(seed)

    if class_label is None:
        # Subsample from entire dataset
        total_samples = int(data.shape[1] * fraction)
        indices = np.random.choice(data.shape[1], total_samples, replace=False)
    else:
        # Subsample only from specified class
        class_indices = np.where(labels == class_label)[0]
        total_samples = int(len(class_indices) * fraction)
        indices = np.random.choice(class_indices, total_samples, replace=False)

    # Create new dataset excluding selected indices
    mask = np.ones(data.shape[1], dtype=bool)
    mask[indices] = False
    return data[:, mask], labels[mask]


def subsample_conditional(
    classA: np.ndarray,
    classB: np.ndarray,
    labels_A: np.ndarray,
    labels_B: np.ndarray,
    total_fraction: float = 0.25,
    fraction1: float = 0.2,
    fraction2: float = 0.8,
    seed: int = 42,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Subset 1 here refers to the subset of class A where X1 < 0.
    Meanwhile subset 2 refers to the subset of class A where X1 > 0.
    The goal of this function is to remove a random 25% of all the data samples
    where of the 25% removed datapoints, 20% comes from subset 1 and 80% comes from subset 2.
    """

    np.random.seed(seed)

    subset1_mask = classA[0] < 0
    subset2_mask = classA[0] > 0

    subset1 = classA[:, subset1_mask]
    subset2 = classA[:, subset2_mask]

    subset1_labels = labels_A[subset1_mask]
    subset2_labels = labels_A[subset2_mask]

    subset1_fraction = int(fraction1 * total_fraction * classA.shape[1])
    subset2_fraction = int(fraction2 * total_fraction * classA.shape[1])

    # Randomly select indices for subset2 (20%)
    subset1_indices = np.random.choice(
        subset1.shape[1], subset1_fraction, replace=False
    )
    subset1_remaining = np.delete(subset1, subset1_indices, axis=1)
    subset1_remaining_labels = np.delete(subset1_labels, subset1_indices)

    # Randomly select indices for subset2 (80%)
    subset2_indices = np.random.choice(
        subset2.shape[1], subset2_fraction, replace=False
    )
    subset2_remaining = np.delete(subset2, subset2_indices, axis=1)
    subset2_remaining_labels = np.delete(subset2_labels, subset2_indices)

    # Remove additional 25% * 20% of subset1 and 25% * 80% of subset2
    subset1_remove_count = int(0.25 * 0.2 * classA.shape[1])
    subset2_remove_count = int(0.25 * 0.8 * classA.shape[1])

    subset1_remove_indices = np.random.choice(
        subset1_remaining.shape[1], subset1_remove_count, replace=False
    )
    subset1_final = np.delete(subset1_remaining, subset1_remove_indices, axis=1)
    subset1_final_labels = np.delete(subset1_remaining_labels, subset1_remove_indices)

    subset2_remove_indices = np.random.choice(
        subset2_remaining.shape[1], subset2_remove_count, replace=False
    )
    subset2_final = np.delete(subset2_remaining, subset2_remove_indices, axis=1)
    subset2_final_labels = np.delete(subset2_remaining_labels, subset2_remove_indices)

    # Concatenate remaining data from subset1 and subset2
    classA_final = np.hstack((subset1_final, subset2_final))
    labels_A_final = np.hstack((subset1_final_labels, subset2_final_labels))

    # Concatenate with class B
    final_data = np.hstack((classA_final, classB))
    final_labels = np.hstack((labels_A_final, labels_B))

    return final_data, final_labels


def evaluate_model(
    model: Union[DeltaRuleClassifier, PerceptronClassifier],
    data: np.ndarray,
    labels: np.ndarray,
):
    preds = model.predict(data).flatten()

    true_positives = np.sum((preds == 1) & (labels == 1))
    true_negatives = np.sum((preds != 1) & (labels != 1))
    false_positives = np.sum((preds == 1) & (labels != 1))
    false_negatives = np.sum((preds != 1) & (labels == 1))

    accuracy = np.mean(preds == labels)
    sensitivity = (
        true_positives / (true_positives + false_negatives)
        if (true_positives + false_negatives) > 0
        else 0
    )
    specificity = (
        true_negatives / (true_negatives + false_positives)
        if (true_negatives + false_positives) > 0
        else 0
    )

    return accuracy, sensitivity, specificity
