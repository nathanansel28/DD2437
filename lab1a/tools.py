from typing import List, Tuple

import matplotlib.pyplot as plt
import numpy as np


def get_linearly_separable_dataset(
    n_samples: int = 100,
    means: np.ndarray = np.array([[4.0, -2.0], [-2.0, 3.0]]),
    standard_deviations: List[float] = [3.5, 5.0],
    seed: int = 20250122,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Generates a synthetic, linearly separable dataset with two classes,
    where each class follows a Gaussian-like distribution.

    Parameters:
    ----------
    n_samples : int, optional
        The number of samples to generate per class. Default is 100.
    means : np.ndarray, optional
        A 2D array where each row represents the mean (center) of a class
        in 2D space. Default is np.array([[4.0, -2.0], [-2.0, 3.0]]).
    standard_deviations : List[float], optional
        A list of standard deviations for each class. Each standard deviation
        determines the spread of the points around the corresponding mean.
        Default is [3.5, 5.0].
    seed : int, optional
        Random seed for reproducibility. Default is 20250122.

    Returns:
    -------
    Tuple[np.ndarray, np.ndarray]
        A tuple containing:
        - data: A 2D NumPy array of shape (n_samples * num_classes, 2), where
          each row is a point in 2D space.
        - labels: A 1D NumPy array of shape (n_samples * num_classes,),
          where each element is the class label (0, 1, ...).

    Raises:
    ------
    AssertionError
        If the number of class means does not match the number of standard
        deviations or if `means` is not a 2D array.

    Notes:
    -----
    - The data points for each class are generated by adding Gaussian noise
      to the specified class mean, scaled by the corresponding standard
      deviation.
    - The generated data and labels are shuffled to ensure randomness.
    """

    # Ensure two-dimensionality
    assert means.ndim == 2
    # Ensure same amount of classes
    assert len(means) == len(standard_deviations)

    # Initialise data points and labels
    data = []
    labels = []
    generator = np.random.default_rng(seed=seed)

    for i in range(len(means)):
        # Generate random points with specific mean and standard deviation
        points = generator.random((2, n_samples)) * standard_deviations[i] + means[
            i
        ].reshape(2, 1)
        # Add points to data
        data.append(points)
        # Add same label for the generated points
        labels.append(np.full((n_samples), i, dtype=int))

    # Concatenate all data points and labels
    data = np.hstack(data)
    labels = np.hstack(labels)

    # Shuffle data points and labels with same permutation
    indexes = generator.permutation(n_samples * len(means))
    data = data[:, indexes]
    labels = labels[indexes]

    return data, labels


def plot_decision_boundary(
    weights: np.ndarray, data: np.ndarray, labels: np.ndarray, title: str
):
    """
    Plots a 2D dataset with its corresponding decision boundary.

    Parameters:
    ----------
    weights : np.ndarray
        A 1D NumPy array of shape (3,) or (,3) containing the weights of the decision
        boundary equation: [w1, w2, b], where:
        - w1 and w2 are the coefficients for the features X1 and X2.
        - b is the bias term.
    data : np.ndarray
        A 2D NumPy array of shape (2, n_samples) containing the dataset. Each
        column represents a data point, where the first row contains the X1
        coordinates, and the second row contains the X2 coordinates.
    labels : np.ndarray
        A 1D NumPy array of shape (n_samples,) containing the class labels
        (0 or 1) for each data point.
    title : str
        The title of the plot.

    Returns:
    -------
    None
        This function does not return anything. It displays the plot.

    Notes:
    -----
    - The decision boundary is defined by the equation: `w1 * X1 + w2 * X2 + b = 0`.
      If w2 is 0, a vertical line is plotted at `X1 = -b / w1`.
    - The dataset points are colored based on their labels:
        - "Class A" (label 0) is displayed in blue.
        - "Class B" (label 1) is displayed in orange.
    - The plot includes axes with gridlines for better visualization.
    """
    # Unpack weights (w1, w2, b)
    w1, w2, b = weights

    # Generate X1 values (spanning the range of the dataset)
    x1_range = np.linspace(data[0, :].min() - 1, data[0, :].max() + 1, 100)

    # Calculate x2 values using the decision boundary equation
    if w2 != 0:  # Avoid division by zero
        x2_range = -(w1 * x1_range + b) / w2
    else:
        x2_range = np.full_like(x1_range, -b / w1)  # Vertical line if w2 == 0

    # Plot the data points
    plt.scatter(
        data[0, labels == 0],
        data[1, labels == 0],
        label="Class A",
        alpha=0.7,
        color="blue",
    )
    plt.scatter(
        data[0, labels == 1],
        data[
            1,
            labels == 1,
        ],
        label="Class B",
        alpha=0.7,
        color="orange",
    )

    # Plot the decision boundary
    plt.plot(x1_range, x2_range, "k--", label="Decision boundary")

    # Customize the plot
    plt.axhline(0, color="gray", linestyle="--", linewidth=0.5)
    plt.axvline(0, color="gray", linestyle="--", linewidth=0.5)
    plt.xlabel("X1")
    plt.ylabel("X2")
    plt.legend()
    plt.title(title)
    plt.tight_layout()
    plt.show()


class PerceptronClassifier:
    def __init__(self) -> None:
        self.weights: np.ndarray = None

    def __add_bias(self, X: np.ndarray) -> np.ndarray:
        return np.vstack((X, np.ones((1, X.shape[1]))))

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        learn_rate: float = 0.25,
        epochs: int = 10,
        batch: bool = False,
    ) -> None:
        inputs = self.__add_bias(X)
        labels = y.reshape((-1, 1))

        rndg = np.random.default_rng(seed=20250122)
        self.weights = rndg.random((len(X) + 1, 1)) * 0.1 - 0.05
        accumulator = np.zeros_like(self.weights)

        for _ in range(epochs):
            preds = self.predict(X)
            delta = learn_rate * np.dot(inputs, labels - preds)

            if batch:
                accumulator += delta
            else:
                self.weights += delta

        if batch:
            self.weights += accumulator

    def predict(self, X: np.ndarray) -> np.ndarray:
        inputs = self.__add_bias(X)

        # Compute activations
        activations = np.dot(np.transpose(self.weights), inputs)
        # Threshold the activations
        return np.where(activations > 0, 1, 0).reshape((-1, 1))
